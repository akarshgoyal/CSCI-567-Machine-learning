{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Updated @ 02/06) Problem 2: K-nearest neighbor (KNN) for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes\n",
    "\n",
    "In this task, we will use three distance functions: (we removed the vector symbol for simplicity)\n",
    "\n",
    "- Euclidean distance:  $$d(x, y) = \\sqrt{\\langle x - y, x - y \\rangle}$$\n",
    "- Inner product distance: $$d(x, y ) = \\langle x, y \\rangle$$\n",
    "- Gaussian kernel distance: \n",
    "    $$d(x, y ) = - \\exp({−\\frac 12 || x - y ||^2}) $$\n",
    "\n",
    "\n",
    "F1-score is a important metric for binary classification, as sometimes the accuracy metric has the false positive (a good example is in MLAPP book 2.2.3.1 “Example: medical diagnosis”, Page 29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 Distance Functions\n",
    "\n",
    "Implement the class in file *hw1_knn.py*\n",
    "    - KNN\n",
    "    \n",
    "and the functions in *utils.py*    \n",
    "    - f1_score\n",
    "    - euclidean_distance\n",
    "    - inner_product_distance\n",
    "    - gaussian_kernel_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hw1_knn import KNN\n",
    "from utils import euclidean_distance, gaussian_kernel_distance, inner_product_distance\n",
    "from utils import f1_score\n",
    "\n",
    "distance_funcs = {\n",
    "    'euclidean': euclidean_distance,\n",
    "    'gaussian': gaussian_kernel_distance,\n",
    "    'inner_prod': inner_product_distance,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing \n",
    "\n",
    "Do the following steps:\n",
    "\n",
    "- Load data (features and values) from function generate data cancer\n",
    "- Check that there are 569 data samples and each sample have a feature vector of length 30.\n",
    "- Split the whole data set into three parts:\n",
    "     - the train set contains first 400 samples (0th - 399th samples),\n",
    "     - the validation set contains the next 60 samples (400th - 459th samples),\n",
    "     - the test set contains the rest 109 samples (460th - 568th samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import generate_data_cancer\n",
    "features, labels = generate_data_cancer()\n",
    "\n",
    "train_features, train_labels = features[:400], labels[:400]\n",
    "valid_features, valid_labels = features[400:460], labels[400:460]\n",
    "test_features, test_labels = features[460:], labels[460:]\n",
    "\n",
    "assert len(train_features) == len(train_labels) == 400\n",
    "assert len(valid_features) == len(valid_labels) == 60\n",
    "assert len(test_features) == len(test_labels) == 109"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection \n",
    "In kNN model, the parameter k is a hyper-parameter. In this task, we only search k among {1, 3, 10, 20, 50}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, func in distance_funcs.items():\n",
    "    best_f1_score, best_k = -1, 0\n",
    "    for k in [1, 3, 10, 20, 50]:\n",
    "        model = KNN(k=k, distance_function=func)\n",
    "        model.train(train_features, train_labels)\n",
    "        train_f1_score = f1_score(\n",
    "            train_labels, model.predict(train_features))\n",
    "\n",
    "        valid_f1_score = f1_score(\n",
    "            valid_labels, model.predict(valid_features))\n",
    "        print('[part 2.1] {name}\\tk: {k:d}\\t'.format(name=name, k=k) + \n",
    "              'train: {train_f1_score:.5f}\\t'.format(train_f1_score=train_f1_score) +\n",
    "              'valid: {valid_f1_score:.5f}'.format(valid_f1_score=valid_f1_score))\n",
    "\n",
    "        if valid_f1_score > best_f1_score:\n",
    "            best_f1_score, best_k = valid_f1_score, k\n",
    "\n",
    "    model = KNN(k=best_k, distance_function=func)\n",
    "    model.train(train_features + valid_features,\n",
    "                train_labels + valid_labels)\n",
    "    test_f1_score = f1_score(test_labels, model.predict(test_features))\n",
    "    print()\n",
    "    print('[part 2.1] {name}\\tbest_k: {best_k:d}\\t'.format(name=name, best_k=best_k) +\n",
    "          'test f1 score: {test_f1_score:.5f}'.format(test_f1_score=test_f1_score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 Data transformation\n",
    "\n",
    "We are going to add one more step (data transformation) in the data processing part and see how it works. \n",
    "Sometimes, normalization plays an important role to make a machine learning model work (check term “Feature scaling” in wiki).\n",
    "\n",
    "Here, we take two different data transformation approaches.\n",
    "\n",
    "#### Normalizing the feature vector \n",
    "\n",
    "This one is simple but some times may work well. Given a feature vector $x$, the normalized feature vector is given by \n",
    "\n",
    "$$ x' = \\frac x {\\sqrt{\\langle x, x \\rangle}} $$\n",
    "If a vector is a all-zero vector, we let the normalized vector also be a all-zero vector.\n",
    "\n",
    "\n",
    "#### Min-max scaling the feature matrix\n",
    "\n",
    "The above normalization is data independent, that is to say, the output of the normalization function doesn’t depend on the rest training data. However, sometimes it would be helpful to do data dependent normalization. One thing to note is that, when doing data dependent normalization, we can only use training data, as the test data is assumed to be unknown during training (at least for most classification tasks).\n",
    "\n",
    "The min-max scaling works as follows: after min-max scaling, all values of training data’s feature vectors are in the given range.\n",
    "Note that this doesn’t mean the values of the validation/test data’s fea- tures are all in that range, because the validation/test data may have dif- ferent distribution as the training data.\n",
    "\n",
    "> Reference: https://en.wikipedia.org/wiki/Feature_scaling\n",
    "\n",
    "> For short, you should keep the global max and min in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** the classes in *utils.py*    \n",
    "    - NormalizationScaler\n",
    "    - MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import NormalizationScaler, MinMaxScaler\n",
    "\n",
    "scaling_functions = {\n",
    "    'min_max_scale': MinMaxScaler,\n",
    "    'normalize': NormalizationScaler,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "\n",
    "Repeat the model selection part in part 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for scaling_name, scaling_class in scaling_functions.items():\n",
    "    for name, func in distance_funcs.items():\n",
    "        scaler = scaling_class()\n",
    "        train_features_scaled = scaler(train_features)\n",
    "        valid_features_scaled = scaler(valid_features)\n",
    "\n",
    "        best_f1_score, best_k = 0, -1\n",
    "        for k in [1, 3, 10, 20, 50]:\n",
    "            model = KNN(k=k, distance_function=func)\n",
    "            model.train(train_features_scaled, train_labels)\n",
    "            train_f1_score = f1_score(\n",
    "                train_labels, model.predict(train_features_scaled))\n",
    "            \n",
    "            valid_f1_score = f1_score(\n",
    "                valid_labels, model.predict(valid_features_scaled))\n",
    "            print('[part 2.2] {name}\\t{scaling_name}\\tk: {k:d}\\t'.format(name=name, scaling_name=scaling_name, k=k) +\n",
    "                  'train: {train_f1_score:.5f}\\t'.format(train_f1_score=train_f1_score) + \n",
    "                  'valid: {valid_f1_score:.5f}'.format(valid_f1_score=valid_f1_score))\n",
    "\n",
    "            if valid_f1_score > best_f1_score:\n",
    "                best_f1_score, best_k = valid_f1_score, k\n",
    "    \n",
    "\n",
    "        # now change it to new scaler, since the training set changes\n",
    "        scaler = scaling_class()\n",
    "        combined_features_scaled = scaler(train_features + valid_features)\n",
    "        test_features_scaled = scaler(test_features)\n",
    "\n",
    "        model = KNN(k=best_k, distance_function=func)\n",
    "        model.train(combined_features_scaled, train_labels + valid_labels)\n",
    "        test_f1_score = f1_score(test_labels, model.predict(test_features_scaled))\n",
    "        print()\n",
    "        print('[part 2.2] {name}\\t{scaling_name}\\t'.format(name=name, scaling_name=scaling_name) +\n",
    "              'best_k: {best_k:d}\\ttest: {test_f1_score:.5f}'.format(best_k=best_k, test_f1_score=test_f1_score))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
